dataset = dict(
    data=None,
    data_dir=None,
    dataset="",
    train_split="train",
    val_split="validation",
    train_num_samples=None,
    val_num_samples=None,
    dataset_download=False,
    class_map="",
    input_img_mode=None,
    input_key=None,
    target_key=None,
    dataset_trust_remote_code=False
)
model = dict(
    model=dict(
        type='timm.models.resnet.resnet18'
    ),
    pretrained=False,
    pretrained_path=None,
    initial_checkpoint="",
    resume="",
    no_resume_opt=False,
    num_classes=None,
    gp=None,
    img_size=None,
    in_chans=None,
    input_size=None,
    crop_pct=None,
    mean=None,
    std=None,
    interpolation="",
    batch_size=128,
    validation_batch_size=None,
    channels_last=False,
    fuser="",
    grad_accum_steps=1,
    grad_checkpointing=False,
    fast_norm=False,
    model_kwargs={},
    head_init_scale=None,
    head_init_bias=None,
    torchcompile_mode=None,
    torchscript=False,
    torchcompile=None
)
device = dict(
    device="cuda",
    amp=False,
    amp_dtype="float16",
    amp_impl="native",
    model_dtype=None,
    no_ddp_bb=False,
    synchronize_step=False,
    local_rank=0,
    device_modules=None
)
optimizer = dict(
    opt="sgd",
    opt_eps=None,
    opt_betas=None,
    momentum=0.9,
    weight_decay=2e-05,
    clip_grad=None,
    clip_mode="norm",
    layer_decay=None,
    opt_kwargs={}
)
lr_schedule = dict(
    sched="cosine",
    sched_on_updates=False,
    lr=None,
    lr_base=0.1,
    lr_base_size=256,
    lr_base_scale="",
    lr_noise=None,
    lr_noise_pct=0.67,
    lr_noise_std=1.0,
    lr_cycle_mul=1.0,
    lr_cycle_decay=0.5,
    lr_cycle_limit=1,
    lr_k_decay=1.0,
    warmup_lr=0.1,
    min_lr=0,
    epochs=300,
    epoch_repeats=0.0,
    start_epoch=None,
    decay_milestones=[90, 180, 270],
    decay_epochs=90,
    warmup_epochs=5,
    warmup_prefix=False,
    cooldown_epochs=0,
    patience_epochs=10,
    decay_rate=0.1
)
aug_and_reg = dict(
    no_aug=False,
    train_crop_mode=None,
    scale=[0.08, 1.0],
    ratio=[0.75, 1.3333333333333333],
    hflip=0.5,
    vflip=0.0,
    color_jitter=0.4,
    color_jitter_prob=None,
    grayscale_prob=None,
    gaussian_blur_prob=None,
    aa=None,
    aug_repeats=0,
    aug_splits=0,
    jsd_loss=False,
    bce_loss=False,
    bce_sum=False,
    bce_target_thresh=None,
    bce_pos_weight=None,
    reprob=0.0,
    remode="pixel",
    recount=1,
    resplit=False,
    mixup=0.0,
    cutmix=0.0,
    cutmix_minmax=None,
    mixup_prob=1.0,
    mixup_switch_prob=0.5,
    mixup_mode="batch",
    mixup_off_epoch=0,
    smoothing=0.1,
    train_interpolation="random",
    drop=0.0,
    drop_connect=None,
    drop_path=None,
    drop_block=None
)
batch_norm = dict(
    bn_momentum=None,
    bn_eps=None,
    sync_bn=False,
    dist_bn="reduce",
    split_bn=False
)
model_ema = dict(
    model_ema=False,
    model_ema_force_cpu=False,
    model_ema_decay=0.9998,
    model_ema_warmup=False
)
miscellaneous = dict(
    seed=42,
    worker_seeding="all",
    log_interval=50,
    recovery_interval=0,
    checkpoint_hist=10,
    workers=4,
    save_images=False,
    pin_mem=False,
    no_prefetcher=False,
    output="",
    experiment="",
    eval_metric="top1",
    tta=0,
    use_multi_epochs_loader=False,
    log_wandb=False,
    wandb_project=None,
    wandb_tags=[],
    wandb_resume_id=""
)

config_dict = dict(
    **dataset,
    **model,
    **device,
    **optimizer,
    **lr_schedule,
    **aug_and_reg,
    **batch_norm,
    **model_ema,
    **miscellaneous,
)
